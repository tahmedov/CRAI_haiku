{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztoOmrAjuL82"
      },
      "source": [
        "# **Haiku Generator**\n",
        "## Cem Ayerdem and Tolunay Ahmedov\n",
        "\n",
        "User input is used to generate a unique Haiku with the users theme/words in mind\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4J9LF0dXJsU",
        "outputId": "bafbc346-7712-4d8e-8cc8-812f4fc45792"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pronouncing in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: cmudict>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pronouncing) (1.0.16)\n",
            "Requirement already satisfied: importlib-metadata>=5 in /usr/local/lib/python3.10/dist-packages (from cmudict>=0.4.0->pronouncing) (7.0.1)\n",
            "Requirement already satisfied: importlib-resources>=5 in /usr/local/lib/python3.10/dist-packages (from cmudict>=0.4.0->pronouncing) (6.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=5->cmudict>=0.4.0->pronouncing) (3.17.0)\n",
            "Unique words in the dataset:\n",
            "['cooking', 'pension', 'rainbows', 'plan', 'only', 'north', 'streetlamps', 'that', 'solar', 'i', 'am', 'sealed', 'dusk', 'he', 'ageing', 'off', 'chemo', 'point', 'cleaning', 'sparrows', 'gray', 'betelgeuse', 'lifting', 'sunlight', 'rise', 'skin', 'day', 'place', 'prairie', 'sudden', 'her', 'water', 'still', 'willows', 'meet', 'thin', 'rain', 'hole', 'burning', 'gold', 'guest', 'sipping', 'pause', 'roses', 'dont', 'bay', 'strategic', 'side', 'recall', 'boats', 'call', 'just', 'small', 'warmth', 'long', 'homeland', 'anchor', 'trails', 'bag', 'distance', 'early', 'sand', 'beside', 'local', 'frames', 'market', 'spring', 'among', 'window', 'high', 'strides', 'land', 'then', 'eases', 'winter', 'room', 'trail', 'flare', 'both', 'feathering', 'veiled', 'smile', 'my', 'halfhidden', 'head', 'the', 'top', 'february', 'warm', 'we', 'before', 'turning', 'out', 'warmedover', 'light', 'or', 'sends', 'left', 'two', 'fog']\n",
            "Epoch 1/20\n",
            "15/15 [==============================] - 13s 393ms/step - loss: 3.0643\n",
            "Epoch 2/20\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.9145\n",
            "Epoch 3/20\n",
            "15/15 [==============================] - 7s 433ms/step - loss: 2.9152\n",
            "Epoch 4/20\n",
            "15/15 [==============================] - 6s 374ms/step - loss: 2.9183\n",
            "Epoch 5/20\n",
            "15/15 [==============================] - 7s 443ms/step - loss: 2.9179\n",
            "Epoch 6/20\n",
            "15/15 [==============================] - 5s 349ms/step - loss: 2.9026\n",
            "Epoch 7/20\n",
            "15/15 [==============================] - 7s 445ms/step - loss: 2.9046\n",
            "Epoch 8/20\n",
            "15/15 [==============================] - 5s 349ms/step - loss: 2.9024\n",
            "Epoch 9/20\n",
            "15/15 [==============================] - 6s 389ms/step - loss: 2.9022\n",
            "Epoch 10/20\n",
            "15/15 [==============================] - 6s 392ms/step - loss: 2.8924\n",
            "Epoch 11/20\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.9033\n",
            "Epoch 12/20\n",
            "15/15 [==============================] - 7s 448ms/step - loss: 2.8923\n",
            "Epoch 13/20\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.8942\n",
            "Epoch 14/20\n",
            "15/15 [==============================] - 7s 442ms/step - loss: 2.8942\n",
            "Epoch 15/20\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 2.8984\n",
            "Epoch 16/20\n",
            "15/15 [==============================] - 6s 405ms/step - loss: 2.8750\n",
            "Epoch 17/20\n",
            "15/15 [==============================] - 6s 372ms/step - loss: 2.8853\n",
            "Epoch 18/20\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.8806\n",
            "Epoch 19/20\n",
            "15/15 [==============================] - 7s 444ms/step - loss: 2.8776\n",
            "Epoch 20/20\n",
            "15/15 [==============================] - 5s 349ms/step - loss: 2.8883\n",
            "Hi Writer! Welcome to the flexible Haiku generator made by Cem Ayerdem and Tolunay Ahmedov! \n",
            " Enter words to include in the Haikus (comma-separated): snow, rain, beauty, nature\n",
            "Your unique Haiku:\n",
            "the silence between us nature snow filled with sunlight\n"
          ]
        }
      ],
      "source": [
        "!pip install pronouncing\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import re\n",
        "import random\n",
        "\n",
        "# Function to generate Haiku's\n",
        "def generate_haiku(model, seed, max_length=max_seq_length):\n",
        "    seed = seed.ljust(max_length)[:max_length]\n",
        "\n",
        "    result, pattern = [], [char_to_int[char] if char in char_to_int else char_to_int[' '] for char in seed.lower()]\n",
        "\n",
        "    while len(result) < max_length:\n",
        "        x = np.reshape(pattern, (1, len(pattern), 1))\n",
        "        x = x / float(n_vocab)\n",
        "        prediction = model.predict(x, verbose=0)\n",
        "        index = np.argmax(prediction)\n",
        "        result.append(int_to_char[index])\n",
        "        pattern.append(index)\n",
        "        pattern = pattern[1:]\n",
        "\n",
        "    return ''.join(result)\n",
        "\n",
        "# Function to extract unique words from the dataset (inspiration)\n",
        "def get_dataset_words(processed_haikus, num_words=100):\n",
        "    words = set()\n",
        "    for haiku in processed_haikus:\n",
        "        words.update(haiku.split())\n",
        "    return list(words)[:num_words]\n",
        "\n",
        "processed_haikus = [\" \".join(re.sub(r'[^a-z\\s]', '', haiku).split()) for haiku in haikus]\n",
        "\n",
        "# Extract words from the processed Haiku's\n",
        "unique_words = get_dataset_words(processed_haikus, num_words=100)\n",
        "\n",
        "# Print the unique words from random Haiku's\n",
        "print(\"Unique words in the dataset:\")\n",
        "print(unique_words)\n",
        "\n",
        "\n",
        "# \n",
        "# Create a seed sequence from user and dataset words\n",
        "def create_seed_sequence(user_words, dataset_words, num_user_words=2, num_dataset_words=2):\n",
        "    selected_user_words = random.sample(user_words, min(num_user_words, len(user_words)))\n",
        "    selected_dataset_words = random.sample(dataset_words, num_dataset_words)\n",
        "\n",
        "    seed_words = selected_user_words + selected_dataset_words\n",
        "    random.shuffle(seed_words)\n",
        "\n",
        "    return ' '.join(seed_words)\n",
        "\n",
        "# Read and split the Haiku's\n",
        "with open(\"haiku.txt\", \"r\") as file:\n",
        "    haikus = file.read().lower().split(\"\\n\\n\")\n",
        "\n",
        "# Process Haiku\n",
        "processed_haikus = [\" \".join(re.sub(r'[^a-z\\s]', '', haiku).split()) for haiku in haikus]\n",
        "\n",
        "# Determine fixed sequence length\n",
        "max_seq_length = 40\n",
        "\n",
        "# Create unique character map\n",
        "chars = sorted(list(set(\" \".join(processed_haikus))))\n",
        "char_to_int = {char: i for i, char in enumerate(chars)}\n",
        "int_to_char = {i: char for i, char in enumerate(chars)}\n",
        "\n",
        "# Create training data\n",
        "X_data, y_data = [], []\n",
        "for haiku in processed_haikus:\n",
        "    for i in range(0, len(haiku) - max_seq_length, 1):\n",
        "        X = haiku[i:i + max_seq_length]\n",
        "        y = haiku[i + max_seq_length] if i + max_seq_length < len(haiku) else ' '\n",
        "        X_data.append([char_to_int[char] for char in X])\n",
        "        y_data.append(char_to_int[y])\n",
        "\n",
        "n_patterns = len(X_data)\n",
        "n_vocab = len(chars)\n",
        "\n",
        "# Reshape and normalize the input data\n",
        "X = np.reshape(X_data, (n_patterns, max_seq_length, 1))\n",
        "X = X / float(n_vocab)\n",
        "\n",
        "y = to_categorical(y_data, num_classes=n_vocab)\n",
        "\n",
        "# Define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(max_seq_length, 1), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(n_vocab, activation='softmax'))\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
        "\n",
        "# Train model\n",
        "model.fit(X, y, epochs=20, batch_size=64)\n",
        "\n",
        "# Function to  modify a Haiku with user-input words\n",
        "def modify_haiku_with_user_words(haikus, user_words, num_words_to_add=2):\n",
        "    # Randomly select a Haiku\n",
        "    selected_haiku = random.choice(haikus)\n",
        "    words = selected_haiku.split()\n",
        "\n",
        "    # Insert user words\n",
        "    for _ in range(min(num_words_to_add, len(user_words))):\n",
        "        insert_pos = random.randint(0, len(words))\n",
        "        words.insert(insert_pos, random.choice(user_words))\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Read and split\n",
        "with open(\"haiku.txt\", \"r\") as file:\n",
        "    haikus = file.read().lower().split(\"\\n\\n\")\n",
        "\n",
        "# Process Haiku's\n",
        "processed_haikus = [\" \".join(re.sub(r'[^a-z\\s]', '', haiku).split()) for haiku in haikus]\n",
        "\n",
        "# User input for words\n",
        "user_input = input(\"Hi Writer! Welcome to the flexible Haiku generator made by Cem Ayerdem and Tolunay Ahmedov! \\n Enter words to include in the Haikus (comma-separated): \")\n",
        "user_words = user_input.replace(',', ' ').split()\n",
        "\n",
        "# Build modded Haiku\n",
        "modified_haiku = modify_haiku_with_user_words(processed_haikus, user_words)\n",
        "\n",
        "print(\"Your unique Haiku:\")\n",
        "print(modified_haiku)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
